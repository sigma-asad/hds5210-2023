{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigma-asad/hds5210-2023/blob/main/week14_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bQ28GqWAwpe",
        "nbgrader": {
          "grade": false,
          "grade_id": "intro",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Working with multiple data sets\n",
        "\n",
        "There are two data files that we'll be working with for this week's assignment.  They are described below.  Load those data files in with Pandas and then work to answering each of the questions below.  All of these files are found in our usual s3 bucket: `https://hds5210-data.s3.amazonaws.com`\n",
        "\n",
        "## npidata.csv\n",
        "\n",
        "This file is basic information about every healthcare provider in the US.  It has one row for each NPI (National Provider Identifier).  It contains information such as the provider's name and address.\n",
        "\n",
        "* https://hds5210-data.s3.amazonaws.com/npidata.csv\n",
        "\n",
        "\n",
        "## cmsYYYY.csv\n",
        "\n",
        "These are files about what kinds of procedures and patients providers in the US are serving under CMS programs, Medicare and Medicaid.  Each contains various statistics about providers over the course of a year.  There are three of these, for the years 2014, 2015, and 2016.  These files, however, don't contain information about the provider such as where the provider is located.\n",
        "\n",
        "* https://hds5210-data.s3.amazonaws.com/cms2014.csv\n",
        "* https://hds5210-data.s3.amazonaws.com/cms2015.csv\n",
        "* https://hds5210-data.s3.amazonaws.com/cms2016.csv\n",
        "\n",
        "## Our Goals\n",
        "\n",
        "For this assignment, we're going to want to compute some statistics based on the data in the **cms** files, but aggregate that data based on information in the **npidata** file.  As in last week's assignment, you'll need to store your answers in a variable called `answer` at the end of each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B2WNtqb1Awpg",
        "nbgrader": {
          "grade": false,
          "grade_id": "import",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sudfMt65Awpg",
        "nbgrader": {
          "grade": false,
          "grade_id": "01-intro",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## Part 1\n",
        "\n",
        "In this first step, we'll need to merge together all of the **cms** files into a single dataframe.  Be careful that these files might not be identical, so you'll have to look a little bit to figure out how to merge them.\n",
        "\n",
        "As you are merging them, make sure that you retain information about which file (i.e. which year) the data came from.  Call that new columns `year`.\n",
        "\n",
        "In your `answer` variable, provide a complete data frame that contains all of the rows and columns from the **cms** files, plus an additional column to store the year/file that particular row came from.\n",
        "\n",
        "The assertion tests will give you a good idea as to if you're merging the files correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NZgoS86eAwpg",
        "nbgrader": {
          "grade": false,
          "grade_id": "01-solution",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "answer = None\n",
        "\n",
        "### SOLUTION\n",
        "df_1 = pd.read_csv(\"https://hds5210-data.s3.amazonaws.com/cms2014.csv\")\n",
        "df_2 = pd.read_csv(\"https://hds5210-data.s3.amazonaws.com/cms2015.csv\")\n",
        "df_3 = pd.read_csv(\"https://hds5210-data.s3.amazonaws.com/cms2016.csv\")\n",
        "\n",
        "df_1[\"year\"] = 2014\n",
        "df_1_columns = list(df_1.columns)\n",
        "\n",
        "df_2[\"year\"] = 2015\n",
        "df_2_columns = list(df_2.columns)\n",
        "\n",
        "df_3[\"year\"] = 2016\n",
        "df_3_columns = list(df_3.columns)\n",
        "\n",
        "renaming_cols = {}\n",
        "for i in df_1_columns:\n",
        "    renaming_cols[i] = i.lower()\n",
        "df_1 = df_1.rename(columns=renaming_cols)\n",
        "\n",
        "renaming_cols = {}\n",
        "for i in df_2_columns:\n",
        "    renaming_cols[i] = i.lower()\n",
        "df_2 = df_2.rename(columns=renaming_cols)\n",
        "\n",
        "\n",
        "renaming_cols = {}\n",
        "for i in df_3_columns:\n",
        "    renaming_cols[i] = i.lower()\n",
        "df_3 = df_3.rename(columns=renaming_cols)\n",
        "\n",
        "answer = pd.concat([df_1, df_2, df_3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z52BgWZXAwpg",
        "nbgrader": {
          "grade": true,
          "grade_id": "01-tests",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "assert(answer.shape == (193862, 60))\n",
        "assert(list(answer['year'].unique()) == [2014,2015,2016])\n",
        "assert(set(answer.columns.str.lower()) == set(['year', 'nbr', 'npi', 'provider_type',\n",
        "       'medicare_participation_indicator', 'number_of_hcpcs', 'total_services',\n",
        "       'total_unique_benes', 'total_submitted_chrg_amt',\n",
        "       'total_medicare_allowed_amt', 'total_medicare_payment_amt',\n",
        "       'total_medicare_stnd_amt', 'drug_suppress_indicator',\n",
        "       'number_of_drug_hcpcs', 'total_drug_services',\n",
        "       'total_drug_unique_benes', 'total_drug_submitted_chrg_amt',\n",
        "       'total_drug_medicare_allowed_amt', 'total_drug_medicare_payment_amt',\n",
        "       'total_drug_medicare_stnd_amt', 'med_suppress_indicator',\n",
        "       'number_of_med_hcpcs', 'total_med_services', 'total_med_unique_benes',\n",
        "       'total_med_submitted_chrg_amt', 'total_med_medicare_allowed_amt',\n",
        "       'total_med_medicare_payment_amt', 'total_med_medicare_stnd_amt',\n",
        "       'beneficiary_average_age', 'beneficiary_age_less_65_count',\n",
        "       'beneficiary_age_65_74_count', 'beneficiary_age_75_84_count',\n",
        "       'beneficiary_age_greater_84_count', 'beneficiary_female_count',\n",
        "       'beneficiary_male_count', 'beneficiary_race_white_count',\n",
        "       'beneficiary_race_black_count', 'beneficiary_race_api_count',\n",
        "       'beneficiary_race_hispanic_count', 'beneficiary_race_natind_count',\n",
        "       'beneficiary_race_other_count', 'beneficiary_nondual_count',\n",
        "       'beneficiary_dual_count', 'beneficiary_cc_afib_percent',\n",
        "       'beneficiary_cc_alzrdsd_percent', 'beneficiary_cc_asthma_percent',\n",
        "       'beneficiary_cc_cancer_percent', 'beneficiary_cc_chf_percent',\n",
        "       'beneficiary_cc_ckd_percent', 'beneficiary_cc_copd_percent',\n",
        "       'beneficiary_cc_depr_percent', 'beneficiary_cc_diab_percent',\n",
        "       'beneficiary_cc_hyperl_percent', 'beneficiary_cc_hypert_percent',\n",
        "       'beneficiary_cc_ihd_percent', 'beneficiary_cc_ost_percent',\n",
        "       'beneficiary_cc_raoa_percent', 'beneficiary_cc_schiot_percent',\n",
        "       'beneficiary_cc_strk_percent', 'beneficiary_average_risk_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77sjXAf2Awpg",
        "nbgrader": {
          "grade": false,
          "grade_id": "02-intro",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## Part 2\n",
        "\n",
        "In this next part, we're going to join the **cms** data with the provider information in the **https://hds5210-data.s3.amazonaws.com/npidata.csv** file.  In this join, we don't want to lose any records from the **cms** files, even if no matching provider exists in the **npidata** file.  However, we don't care about any providers from the **npidata** file that don't have records in the **cms** files.  Those providers can be ignored.\n",
        "\n",
        "Join the data files together to create one unified data frame called `answer`.  This dataframe should have all the columns from both **cms** and **npidata** files, joined together using the `npi` column.  \n",
        "\n",
        "Note that `npi` is unique in the **npidata** file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCL6OxN0Awph",
        "nbgrader": {
          "grade": false,
          "grade_id": "02-solution",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "npi_data = pd.read_csv(\"https://hds5210-data.s3.amazonaws.com/npidata.csv\")\n",
        "answer = answer.merge(npi_data, how = 'left', left_on = 'npi', right_on = 'NPI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPEM0Bq0Awph",
        "nbgrader": {
          "grade": true,
          "grade_id": "02-tests",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "assert(answer.shape == (193862, 103))\n",
        "assert(list(answer['Provider Business Mailing Address State Name'].unique()) == ['IL','MO',numpy.nan,'WY'])\n",
        "assert(list(answer.groupby('Provider Business Mailing Address State Name').NPI.count()) == [111520, 53366, 4805])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID5cZ8ZZAwph",
        "nbgrader": {
          "grade": false,
          "grade_id": "03-intro",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## Part 3\n",
        "\n",
        "If you did everything right above, you'll notice that grouping by a column with NaN in it will cause some rows to disappear from the aggregation test.  So, let's create a new column called `'State'` that has the same value as whatever is in the `'Provider Business Mailing Address State Name'` column or a value of `'XX'` if there is no state information.\n",
        "\n",
        "Set `answer` to be your final data frame with the new `'State'` column added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgc2cUlrAwph",
        "nbgrader": {
          "grade": false,
          "grade_id": "03-solution",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "answer[\"State\"] = answer['Provider Business Mailing Address State Name']\n",
        "answer[\"State\"] = answer[\"State\"].fillna(\"XX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503H14qTAwph",
        "nbgrader": {
          "grade": true,
          "grade_id": "03-tests",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "assert(list(answer.groupby('State').npi.count()) == [111520, 53366, 4805, 24171])\n",
        "assert(answer.shape == (193862, 104))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1jMpj5Awph",
        "nbgrader": {
          "grade": false,
          "grade_id": "04-intro",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## Part 4\n",
        "\n",
        "Next, let's summarize the data by year and by State.  Create a pivot table that contains one row for each state and one column for each year.  Within the pivot table, put a sum of total services as the values.\n",
        "\n",
        "Assign `answer` to be that resulting pivot table.  In the tests, I'm going to plot a bar chart of your pivot table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hduU6o1XAwph",
        "nbgrader": {
          "grade": false,
          "grade_id": "04-solution",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "values=[\"total_services\"]\n",
        "index=[\"State\"]\n",
        "columns=['year']\n",
        "aggfunc = \"sum\"\n",
        "\n",
        "answer = pd.pivot_table(answer, values = values, index = index, columns = columns, aggfunc=aggfunc)\n",
        "answer = answer.droplevel(0, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMiUBo4BAwph",
        "nbgrader": {
          "grade": true,
          "grade_id": "04-tests",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "assert(answer.shape == (4,3))\n",
        "assert(round(answer.sum().sum(), 1) == 519185664.7)\n",
        "assert(round(answer[2016].sum(), 1) == 176596933.8)\n",
        "assert(answer.loc['WY'].sum() == 10892707.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6snD9sOGAwph"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "answer.plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyfDydeD8EJL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}